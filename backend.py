import os
import cv2
import torch
import torch.nn.functional as F
from torchvision import transforms
from PIL import Image
from facenet_pytorch import MTCNN

# ------------------ C·∫•u h√¨nh ------------------
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"‚úÖ ƒêang s·ª≠ d·ª•ng thi·∫øt b·ªã: {DEVICE}")

MODEL_PATH = "cnn_functional_model.pt"
DB_PATH = "face_database.pt"
THRESHOLD = 1.0  # Ng∆∞·ª°ng nh·∫≠n di·ªán
MAX_IMAGES_PER_PERSON = 20  # Gi·ªõi h·∫°n ·∫£nh m·ªói ng∆∞·ªùi

# ------------------ T·∫£i tr·ªçng s·ªë ------------------
state = torch.load(MODEL_PATH, map_location=DEVICE)
conv_w = state["conv_w"].to(DEVICE)
conv_b = state["conv_b"].to(DEVICE)
fc_w = state["fc_w"].to(DEVICE)
fc_b = state["fc_b"].to(DEVICE)

# ------------------ M·∫°ng CNN thu·∫ßn h√†m ------------------
def forward_cnn(x, conv_w, conv_b, fc_w, fc_b):
    x = F.conv2d(x, conv_w, conv_b, stride=1, padding=1)
    x = F.relu(x)
    x = F.max_pool2d(x, kernel_size=2)
    x = x.view(x.size(0), -1)
    x = torch.matmul(x, fc_w) + fc_b
    x = F.normalize(x, p=2, dim=1)
    return x

# ------------------ Load c∆° s·ªü d·ªØ li·ªáu ------------------
if os.path.exists(DB_PATH):
    database = torch.load(DB_PATH)
    print(f"üìÅ ƒê√£ t·∫£i database v·ªõi {len(database)} ng∆∞·ªùi.")
else:
    database = {}
    print("üìÅ Kh√¥ng t√¨m th·∫•y database, t·∫°o m·ªõi.")

# ------------------ Ti·ªÅn x·ª≠ l√Ω ·∫£nh ------------------
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

# ------------------ MTCNN Detector ------------------
mtcnn = MTCNN(keep_all=True, device=DEVICE)

# ------------------ H√†m tr√≠ch xu·∫•t embedding ------------------
def extract_embedding(face_img_bgr):
    img_rgb = Image.fromarray(cv2.cvtColor(face_img_bgr, cv2.COLOR_BGR2RGB))
    tensor = transform(img_rgb).unsqueeze(0).to(DEVICE)
    with torch.no_grad():
        emb = forward_cnn(tensor, conv_w, conv_b, fc_w, fc_b).squeeze(0)
    return emb

# ------------------ H√†m nh·∫≠n di·ªán ------------------
def recognize(query_emb):
    best_name, best_dist = "Unknown", float('inf')
    for name, tensors in database.items():
        tensors = tensors.to(DEVICE)
        with torch.no_grad():
            embs = forward_cnn(tensors, conv_w, conv_b, fc_w, fc_b)
        dists = torch.norm(embs - query_emb.unsqueeze(0), dim=1)
        min_dist = torch.min(dists).item()
        if min_dist < best_dist:
            best_name, best_dist = name, min_dist
    print(f"üîç Nh·∫≠n di·ªán: {best_name} (kho·∫£ng c√°ch {best_dist:.4f})")
    return (best_name, best_dist) if best_dist < THRESHOLD else ("Unknown", best_dist)

# ------------------ H√†m tr·∫£ v·ªÅ t·∫•t c·∫£ t√™n trong database ------------------
def get_all_names():
    return list(database.keys())

# ------------------ V√≤ng l·∫∑p ch√≠nh ------------------
def main():
    cap = cv2.VideoCapture(0)
    print("üì∑ ƒêang m·ªü camera. Nh·∫•n 'a' ƒë·ªÉ th√™m ng∆∞·ªùi, ESC ƒë·ªÉ tho√°t.")

    while True:
        ret, frame = cap.read()
        if not ret:
            print("‚ùå Kh√¥ng th·ªÉ ƒë·ªçc t·ª´ camera.")
            break

        # Ph√°t hi·ªán khu√¥n m·∫∑t v·ªõi MTCNN
        boxes, _ = mtcnn.detect(frame)
        faces = []

        if boxes is not None:
            for box in boxes:
                x1, y1, x2, y2 = [int(coord) for coord in box]
                faces.append((x1, y1, x2 - x1, y2 - y1))

        # Nh·∫≠n di·ªán v√† hi·ªÉn th·ªã khu√¥n m·∫∑t
        for (x, y, w, h) in faces:
            face_img = frame[y:y+h, x:x+w]
            emb = extract_embedding(face_img)
            name, dist = recognize(emb)
            color = (0, 255, 0) if name != "Unknown" else (0, 0, 255)
            label = f"{name} ({dist:.2f})" if name != "Unknown" else "Unknown"
            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)
            cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)

        # X·ª≠ l√Ω ph√≠m
        key = cv2.waitKey(1) & 0xFF
        if key == 27:  # ESC
            break

        elif key == ord('a'):  # Th√™m ng∆∞·ªùi m·ªõi
            new_name = input("‚úèÔ∏è Nh·∫≠p t√™n ng∆∞·ªùi m·ªõi: ").strip()
            if not new_name:
                print("‚ùå T√™n kh√¥ng h·ª£p l·ªá.")
                continue

            if len(faces) == 0:
                print("‚ùå Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t ƒë·ªÉ th√™m.")
                continue

            added_count = 0
            for (x, y, w, h) in faces:
                face_img = frame[y:y+h, x:x+w]
                img_rgb = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))
                tensor = transform(img_rgb).unsqueeze(0).cpu()

                if new_name in database:
                    if database[new_name].shape[0] >= MAX_IMAGES_PER_PERSON:
                        print(f"‚ö†Ô∏è {new_name} ƒë√£ ƒë·ªß {MAX_IMAGES_PER_PERSON} ·∫£nh.")
                        break
                    database[new_name] = torch.cat([database[new_name], tensor], dim=0)
                else:
                    database[new_name] = tensor
                added_count += 1

            if added_count > 0:
                torch.save(database, DB_PATH)
                print(f"‚úÖ ƒê√£ th√™m {added_count} ·∫£nh cho {new_name}.")
            else:
                print("‚ö†Ô∏è Kh√¥ng th√™m ƒë∆∞·ª£c ·∫£nh n√†o.")

        cv2.imshow("Real-time Face Recognition (MTCNN)", frame)

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
